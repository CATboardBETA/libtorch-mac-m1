diff --git a/aten/src/ATen/mps/MPSAllocator.h b/aten/src/ATen/mps/MPSAllocator.h
index d739e8956d..94ccf0eee4 100644
--- a/aten/src/ATen/mps/MPSAllocator.h
+++ b/aten/src/ATen/mps/MPSAllocator.h
@@ -290,7 +290,7 @@ private:
   constexpr static double default_low_watermark_ratio_discrete = 1.0;
 
   const id<MTLDevice> m_device;
-  std::mutex m_mutex;
+  std::recursive_mutex m_mutex;
   // allocated buffers by device pointer
   ska::flat_hash_map<void*, BufferBlock*> m_allocated_buffers;
   // unallocated cached buffers larger than 1 MB
diff --git a/aten/src/ATen/mps/MPSAllocator.mm b/aten/src/ATen/mps/MPSAllocator.mm
index a40ddd7992..ccc193fde9 100644
--- a/aten/src/ATen/mps/MPSAllocator.mm
+++ b/aten/src/ATen/mps/MPSAllocator.mm
@@ -179,6 +179,16 @@
   return true;
 }
 
+bool trigger_callbacks () {
+  for (const auto& name : MPSAllocatorCallbacksRegistry()->Keys()) {
+    MPSAllocatorCallbacksRegistry()->Create(name)->executeMPSAllocatorCallback(
+      nullptr,
+      IMpsAllocatorCallback::EventType::RELEASED
+    );
+  }
+  return true;
+};
+
 BufferBlock* MPSHeapAllocatorImpl::alloc_buffer_block(size_t size, uint32_t usage)
 {
   TORCH_CHECK(size < m_max_buffer_size, "Invalid buffer size: ", format_size(size));
@@ -202,6 +212,8 @@
     block_found =
         // Attempt allocate
         alloc_buffer(params) ||
+        // Try to release more by calling gc.
+        (trigger_callbacks() && get_free_buffer(params)) ||
         // Free enough available cached blocks to satisfy alloc and retry alloc.
         (release_available_cached_buffers(params) && alloc_buffer(params)) ||
         // Free all cached buffers and retry alloc.
@@ -300,7 +312,7 @@
       pool.heaps_pending_update.insert(heap_block);
       m_mutex.unlock();
       m_stream->addCompletedHandler(^(id <MTLCommandBuffer>) {
-        std::lock_guard<std::mutex> lock(m_mutex);
+        std::lock_guard<std::recursive_mutex> lock(m_mutex);
         // check if the heap block still exists
         if (pool.heaps_pending_update.find(heap_block) != pool.heaps_pending_update.end()) {
           pool.heaps_pending_update.erase(heap_block);
@@ -438,7 +450,7 @@
 // public interface to MPSAllocator
 id<MTLBuffer> MPSHeapAllocatorImpl::malloc(size_t size, uint32_t usage)
 {
-  std::lock_guard<std::mutex> lock(m_mutex);
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
 
   BufferBlock* buffer_block = alloc_buffer_block(size, usage);
   return buffer_block ? buffer_block->buffer : nullptr;
@@ -446,7 +458,7 @@
 
 bool MPSHeapAllocatorImpl::isSharedBuffer(void* ptr)
 {
-  std::lock_guard<std::mutex> lock(m_mutex);
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
 
   BufferBlock *buffer_block = get_allocated_buffer_block(ptr);
   // it's OK for the buffer_block to not exist yet
@@ -457,7 +469,7 @@
 {
   BufferBlock* buffer_block = nullptr;
   {
-    std::lock_guard<std::mutex> lock(m_mutex);
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
 
     buffer_block = alloc_buffer_block(size, UsageFlags::SCALAR);
     if (!buffer_block)
@@ -470,7 +482,7 @@
 
 ssize_t MPSHeapAllocatorImpl::getRequestedBufferSize(void* ptr)
 {
-  std::lock_guard<std::mutex> lock(m_mutex);
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
 
   BufferBlock *buffer_block = get_allocated_buffer_block(ptr);
   if (buffer_block)
@@ -481,7 +493,7 @@
 
 void MPSHeapAllocatorImpl::setBufferShape(void* ptr, const IntArrayRef& shape)
 {
-  std::lock_guard<std::mutex> lock(m_mutex);
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
 
   BufferBlock *buffer_block = get_allocated_buffer_block(ptr);
   TORCH_INTERNAL_ASSERT(buffer_block, "failed to find the buffer ", ptr);
@@ -493,7 +505,7 @@
 
 IntArrayRef MPSHeapAllocatorImpl::getBufferShape(void* ptr)
 {
-  std::lock_guard<std::mutex> lock(m_mutex);
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
 
   BufferBlock *buffer_block = get_allocated_buffer_block(ptr);
   if (buffer_block && buffer_block->shape.size() > 0)
@@ -506,7 +518,7 @@
 {
   BufferBlock *buffer_block = nullptr;
   {
-    std::lock_guard<std::mutex> lock(m_mutex);
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
 
     buffer_block = get_allocated_buffer_block(ptr);
     TORCH_INTERNAL_ASSERT(buffer_block);
@@ -519,14 +531,14 @@
   // we sync the scalar pool manually with completion handler at the time buffer is
   // freed when the MPSScalar instance goes our of scope
   m_stream->addCompletedHandler(^(id <MTLCommandBuffer>) {
-    std::lock_guard<std::mutex> lock(m_mutex);
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
     free_buffer(buffer_block);
   });
 }
 
 void MPSHeapAllocatorImpl::emptyCache()
 {
-  std::lock_guard<std::mutex> lock(m_mutex);
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
   release_cached_buffers();
 }
 
